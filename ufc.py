# -*- coding: utf-8 -*-
"""ufc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A56JWxysr6MW2_IDYcGr2c9_BTVAeZDW
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("maksbasher/ufc-complete-dataset-all-events-1996-2024")

print("Path to dataset files:", path)

import pandas as pd
pdf=pd.read_csv(path+'/UFC dataset/Large set/large_dataset.csv')
pdf.head()

pdf.columns.tolist()

pdf1=pdf.drop(columns=['referee','event_name','r_fighter','b_fighter'])

from sklearn.preprocessing import LabelEncoder

categorical_cols = pdf1.select_dtypes(include=['object']).columns

le = LabelEncoder()

for col in categorical_cols:
    pdf1[col] = le.fit_transform(pdf1[col])

pdf1.head()

"""#Plot histograms for each feature with winner and method"""

import matplotlib.pyplot as plt
import seaborn as sns

for col in pdf1.columns:
    if col not in ['winner', 'method']:
        plt.figure(figsize=(10, 6))

        sns.histplot(data=pdf1, x=col, hue='winner', kde=True, element="step")
        plt.title(f'{col.lower()} vs. winner')
        plt.xlabel(col.lower())
        plt.ylabel('Frequency')
        plt.show()

        plt.figure(figsize=(10, 6))
        sns.histplot(data=pdf1, x=col, hue='method', kde=True, element="step")
        plt.title(f'{col.lower()} vs. method')
        plt.xlabel(col.lower())
        plt.ylabel('Frequency')
        plt.show()

"""##Plot the correlation of all the features with winner"""

import matplotlib.pyplot as plt
import seaborn as sns

correlation_matrix = pdf1.corr()['winner']

plt.figure(figsize=(12, 8))
sns.barplot(x=correlation_matrix.index, y=correlation_matrix.values)
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Correlation with Winner")
plt.title("Correlation of Features with Winner")
plt.tight_layout()
plt.show()

"""##*   Surprisingly, more striking attempts from either side (especially blue) correlate negatively, suggesting volume alone doesn’t ensure victory unless accuracy is also present.


##*   Negative correlation shows that simply attempting or having control time isn’t enough to win—it must be effective or combined with damage.


##*   Taller fighters with longer reach may have a slight edge, though correlation is weak.


##*   Slight negative correlation with age and total strikes absorbed per minute—older fighters or those absorbing more strikes are less likely to win.


##*   More time in control and more striking attempts from the winner correlate positively.

##*   The greater the difference in significant or total strikes landed between fighters, the more likely the winner had a higher strike count.


##*   Knockdowns (kd_diff) also show moderate correlation—more knockdowns, higher win probability.

##Plot the correlation of top 30 features with winner
"""

import matplotlib.pyplot as plt
import seaborn as sns

correlation_matrix = pdf1.corr()['winner']

N = 30
correlation_matrix.pop('winner')
top_features = correlation_matrix.abs().sort_values(ascending=False).head(N).index

top_correlation_matrix = correlation_matrix.loc[top_features]


plt.figure(figsize=(10, 6))
sns.barplot(x=top_correlation_matrix.index, y=top_correlation_matrix.values)
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Correlation with Winner")
plt.title(f"Top {N} Features Correlated with Winner")
plt.tight_layout()
plt.show()

"""#Inferences:


##*   Striking dominance (volume, accuracy, and impact) is a major determinant of winning.


##*   Control time and takedown effectiveness moderately influence outcomes.

##*   The difference-based features (_diff) are much more predictive than absolute metrics.



##*   Corner-specific features suggest that the red corner might be outperforming blue in general (though this could be sample-specific).

##Plot the correlation of all the features with method
"""

import matplotlib.pyplot as plt
import seaborn as sns

correlation_matrix = pdf1.corr()['method']
correlation_matrix.pop('method')
plt.figure(figsize=(12, 8))
sns.barplot(x=correlation_matrix.index, y=correlation_matrix.values)
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Correlation with Method")
plt.title("Correlation of Features with Method")
plt.tight_layout()
plt.show()

"""##*   High values for strikes attempted, takedown attempts, and control time are negatively correlated with finishes.


##*   This pattern indicates that volume and control-based fighting styles, while dominant, often lead to decisions rather than finishes.


##*   Fighters who accumulate points through control and activity are less likely to finish the fight.


##*   Higher submission averages and submission attempt differences increase the likelihood of a submission finish.


##*   These are among the few features with strong positive correlation, indicating that ground specialists are more likely to end fights inside the distance.


##*   Knockdowns are closely associated with finishes by KO/TKO.


##*   High significant strike accuracy also points toward cleaner, more damaging strikes, which are more likely to end fights.


##*   Moderate positive correlation: fighters with greater height and reach might use those advantages to secure finishes, especially in striking exchanges.


##*   However, these correlations are weaker than fight dynamics like knockdowns or submissions.

##Plot the correlation of top 30 features with method
"""

import matplotlib.pyplot as plt
import seaborn as sns

correlation_matrix = pdf1.corr()['method']
correlation_matrix.pop('method')
N = 30
top_features = correlation_matrix.abs().sort_values(ascending=False).head(N).index

top_correlation_matrix = correlation_matrix.loc[top_features]


plt.figure(figsize=(10, 6))
sns.barplot(x=top_correlation_matrix.index, y=top_correlation_matrix.values)
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Correlation with Method")
plt.title(f"Top {N} Features Correlated with Method")
plt.tight_layout()
plt.show()

"""#Inferences:


##*   High-volume fighters using control and consistent activity tend to win by decision, not finish.


##*   Submission metrics (average, attempts, differences) are strong indicators of a likely submission finish.



##*   Knockdowns and significant strike accuracy are reliable signals for KO finishes.


##*   Physical reach and height provide a modest edge in finishing, especially when combined with striking skill.

##Remove the columns which are only availabe after a fight has already happened
"""

pdf2=pdf1.drop(columns=['method','finish_round','time_sec','winner'])

pdf1=pdf1.dropna()

"""##Drop all the rows containing null values

"""

pdf3=pdf2.dropna()
pdf3.isna().sum().sum()

"""##Train a model to predict winner using decision tree

"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,classification_report

X = pdf3
y = pdf1['winner']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier()

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Decision Tree Classifier: {accuracy}")
print(f"Classification Report: {classification_report(y_test,y_pred)}")

"""##Train a model to predict winner using random forest"""

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)

rf_clf.fit(X_train, y_train)

rf_y_pred = rf_clf.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_y_pred)
print(f"Accuracy of the Random Forest Classifier: {rf_accuracy}")
print(f"Classification Report: {classification_report(y_test, rf_y_pred)}")

"""##Train a model to predict winner using gradient boosting

"""

import xgboost as xgb

xgb_clf = xgb.XGBClassifier(random_state=42)

xgb_clf.fit(X_train, y_train)

xgb_y_pred = xgb_clf.predict(X_test)

xgb_accuracy = accuracy_score(y_test, xgb_y_pred)
print(f"Accuracy of the XGBoost Classifier: {xgb_accuracy}")
print(f"Classification Report: {classification_report(y_test, xgb_y_pred)}")

"""##Train a mode to predict winner using naive bayes

"""

from sklearn.naive_bayes import GaussianNB

nb_clf = GaussianNB()

nb_clf.fit(X_train, y_train)

nb_y_pred = nb_clf.predict(X_test)

nb_accuracy = accuracy_score(y_test, nb_y_pred)
print(f"Accuracy of the Naive Bayes Classifier: {nb_accuracy}")
print(f"Classification Report: {classification_report(y_test, nb_y_pred)}")

"""##Train a model to predict winner using k nearest neighbours
:
"""

from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier(n_neighbors=5)

knn_clf.fit(X_train, y_train)

knn_y_pred = knn_clf.predict(X_test)

knn_accuracy = accuracy_score(y_test, knn_y_pred)
print(f"Accuracy of the KNN Classifier: {knn_accuracy}")
print(f"Classification Report: {classification_report(y_test, knn_y_pred)}")

"""##Plot the auc-roc curve for our model"""

from sklearn.metrics import roc_curve, auc

xgb_y_prob = xgb_clf.predict_proba(X_test)[:, 1]

fpr, tpr, _ = roc_curve(y_test, xgb_y_prob)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""##Train a model to predict model using decision tree"""

clf = DecisionTreeClassifier(random_state=42)

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Decision Tree Classifier: {accuracy}")
print(classification_report(y_test, y_pred))

"""##Train a model to predict method using random forest"""

X = pdf3
y = pdf1['method']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

rf_y_pred = rf_clf.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_y_pred)
print(f"Accuracy of the Random Forest Classifier: {rf_accuracy}")
print(classification_report(y_test, rf_y_pred))

"""##Train a model to predict method using naive bayes

"""

nb_clf = GaussianNB()

nb_clf.fit(X_train, y_train)

nb_y_pred = nb_clf.predict(X_test)

nb_accuracy = accuracy_score(y_test, nb_y_pred)
print(f"Accuracy of the Naive Bayes Classifier: {nb_accuracy}")
print(f"Classification Report: {classification_report(y_test, nb_y_pred)}")

"""##Train a model to predict method using k nearest neighbours"""

knn_clf = KNeighborsClassifier(n_neighbors=5)

knn_clf.fit(X_train, y_train)

knn_y_pred = knn_clf.predict(X_test)

knn_accuracy = accuracy_score(y_test, knn_y_pred)
print(f"Accuracy of the KNN Classifier: {knn_accuracy}")

print(f"Classification Report:\n{classification_report(y_test, knn_y_pred)}")

"""##Train a model to predict method using gradient boosting"""

xgb_clf = xgb.XGBClassifier()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_clf = xgb.XGBClassifier(random_state=42)

xgb_clf.fit(X_train, y_train)

xgb_y_pred = xgb_clf.predict(X_test)

xgb_accuracy = accuracy_score(y_test, xgb_y_pred)
print(f"Accuracy of the XGBoost Classifier: {xgb_accuracy}")
print(f"Classification Report: {classification_report(y_test, xgb_y_pred)}")

"""##Plot auc-roc curve for our method model"""

from sklearn.preprocessing import LabelBinarizer

# Get predicted probabilities
y_prob = rf_clf.predict_proba(X_test)

# Convert y_test to one-hot encoding
lb = LabelBinarizer()
y_test_bin = lb.fit_transform(y_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
n_classes = y_prob.shape[1]

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot ROC curves for each class and micro-average
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Some extension of Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")
plt.show()

